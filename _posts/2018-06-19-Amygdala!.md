---
layout: post
title:  "Amygdala!"
date:   2018-06-19 22:19:42 +0800
categories: junk
math: true
---

<!--more-->

#### Difference between static_rnn and dynamic_rnn

`tensorflow.static_rnn` gives the graph of unrolled RNN, w.r.t. the time dimension. Tensorboard will show a stacked architecture with `sequence_length` of `rnn_cell`s. Thus during the feed phase, every batch is required to have the same `sequence_length`, a.k.a. `time_steps`.

`tensorflow.dynamic_rnn` does not unroll RNN. It uses `tf.while_loop` and other control flow ops to generate a graph to execute the loop. For the dynamic version, `sequence_length` means loop numbers and batches can have variable `sequence_length`.

#### NN for regression

When applied to regression problem, the NN's output layer has only one node, the output of the node is the same as the input of the node. That is, linear activation function: \\(f(x) = x\\).

A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an *activation function*.

We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called *forward propagation*.

The weights act as bridges. We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called *back propagation*.

##### RNN for regression

In a mini-batch, each sample has one state (scalar or tuple for LSTM cell). There are `batch_size` parallel states in a layer. When a RNN is not *stateful*, the state is reset after every sample. They don't work by batches, the idea in a batch is that every sample is independent from each other.

- `stateful=False`. Under such setting, all the states are resetted after each batch. After processed some batches:
  - A state is: Where am I now inside a sequence? Which time step is it? How is this particular sequence behaving since its beginning up to now?
  - A weight is: What do I know about the general behavior of all sequences I've seen so far?

- `statefun=True`. In this case, there is also the same number of parallel states, but they will simply not be resetted at all. The next batch with `batch_size` sequences (it's required to remain the same) will reuse the same `batch_size` states that were created before. After feeding many batches in an epoch, the actual meaning is
  - `batch_size` individual sequences, each with length `time_steps * num_batches`
  - When you see that you reached the total length of the sequences, then you call `model.reset_states()`, meaning you will not continue the previous sequences anymore, now you will start feeding new sequences.

#### TensorFlow best practices

## Naming the ops
The way to understand statements like `a = tf.op(..., name='operation_name')` is to break it down to two components.
- First, calling `tf.op(..., name='operation_name')` would indeed lead to the construction of a new operation. The operation is also added to the computational graph.
- Second, though this looks like a constructor, it is just a function call with side-effect, and the return value of this function is a tensor. Most TensorFlow functions return `tf.Tensors`. Some TF functions return `tf.Operations` instead of `tf.Tensors`: `init = tf.global_variables_initializer()` and `train = optimizer.minimize(loss)` are ops. The result of calling run on an Operation is None. You run an operation to cause a side-effect, not to retrieve a value.

Thus, to get the output of an operation, you will have to pass the output tensor to `session.run`, not the operator itself. For instance, use `session.run(op.outputs, feed_dict=...)`.

Usually there will be a `name` parameter when you call the creater of ops. We recommend naming operations that you may need later, so when the graph is restored, you can retrieve these operations by their names, using `graph.get_operation_by_name(name)`. This name is independent of the names the objects are assigned to in Python. Tensors are named after the operation that produces them followed by an output index, as in "add:0" above.

If the tensor is returned from a library call, then you do not get to name the last operation. A hack you can do is to add the tensor to your computation graph's collection. You can put any intermediate operation or tensor into the default graph's collection, via
{% highlight python %}
tf.add_to_collection("tensor_collection_key", tensor)
{% endhighlight %}
later use
{% highlight python %}
tensor = tf.get_collection("tensor_collection_key")[i]
# i is the index remembered
# or graph.get_collection("collection_name")
{% endhighlight %}
to restore the tensor.

But each collection is a list. You will need to either store each value under a unique key, or remember the order values are added to the collection to retrieve them properly. It is far more error-prone than just use unique operation names.

A better way compared to above is to wrap an `identity` operation around the library call, and name the identity operation properly. The code looks like this
{% highlight python %}
# normally you'd just use r = some_library_call(args), if you don't care about r in restored graph.
r = tf.identity(some_library_call(args), name="unique_op_name")
{% endhighlight %}
Now you can easily get the operation or tensor from the graph.

## Feed with TensorFlow Dataset framework

The traditional `feed_dict` and `placeholder` feeding method is *inefficient* and will slow down the training for large, realistic datasets. By using the new `Dataset` API to build high performance data pipelines, all of the operations to load, transform and feed into the model are automatically optimized and paralleled to provide efficient consumption of data.

- Placing input pipeline operations on the CPU can significantly improve performance. Utilizing the CPU for the input pipeline frees the GPU to focus on training.
{% highlight python %}
with tf.device('/cpu:0'):
  distorted_inputs = load_and_process()
{% endhighlight %}


- Using tf.data API
The tf.data API utilizes C++ multi-threading and has a much lower overhead than the Python-based queue_runner that is limited by Python's multi-threading performance. Avoid using feed_dict with large inputs.

1. create the dataset
2. shuffle (with a big enough buffer size)
3. repeat
4. map with parser (preprocessing, augmentation...) using multiple threads
5. batch & prefetch
{%highlight python %}
dataset = dataset.batch(64)
dataset = dataset.prefetch(1)
{% endhighlight %}


- Fused batch norm
Fused batch norm combines the multiple operations needed to do batch normalization into a single kernel. Batch norm is an expensive process that for some models makes up a large percentage of the operation time. Using fused batch norm can result in a 12%-30% speedup.

Deep learning supplies a way to map the raw data space to another feature space in a highly non-linear manner. The correlation which doesn't show up in the original space may appear in the casted image.

E.g. statistical arbitrage opportunity may exsist in price curves, profit curves or volatility curves. After projection, we may observe momentum/reversion.

> **Hint:** You'll need the derivative of the output activation function ($f(x) = x$) for the backpropagation implementation.

Jekyll also offers powerful support for code snippets:

~~~~
This is a
piece of code
in a block
~~~~

{% highlight ruby %}
#!/usr/bin/env/python
# coding=utf-8
import tensorflow as tf
import numpy as np

input_ids = tf.placeholder(dtype=tf.int32, shape=[None])

embedding = tf.Variable(np.identity(5, dtype=np.int32))
input_embedding = tf.nn.embedding_lookup(embedding, input_ids)

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())
print(embedding.eval())
print(sess.run(input_embedding, feed_dict={input_ids:[1, 2, 3, 0, 3, 2, 1]}))
{% endhighlight %}

The known variable `embedding` is a $$5 \times 5$$ diagnal matrix, the placeholder `input_ids` stores the indexes. Above code snippet gives:

~~~
embedding = [[1 0 0 0 0]
             [0 1 0 0 0]
             [0 0 1 0 0]
             [0 0 0 1 0]
             [0 0 0 0 1]]
input_embedding = [[0 1 0 0 0]
                   [0 0 1 0 0]
                   [0 0 0 1 0]
                   [1 0 0 0 0]
                   [0 0 0 1 0]
                   [0 0 1 0 0]
                   [0 1 0 0 0]]
~~~


{% highlight ruby %}
input_embedding = tf.nn.embedding_lookup(embedding, input_ids)
print(sess.run(input_embedding, feed_dict={input_ids:[[1, 2], [2, 1], [3, 3]]}))
{% endhighlight %}

~~~
[[[0 1 0 0 0]
  [0 0 1 0 0]]
 [[0 0 1 0 0]
  [0 1 0 0 0]]
 [[0 0 0 1 0]
  [0 0 0 1 0]]]
~~~


{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

{% highlight python %}
from tensorflow.python.training.moving_averages import assign_moving_average

def batch_norm(x, train, eps=1e-05, decay=0.9, affine=True, name=None):
    with tf.variable_scope(name, default_name='BatchNorm2d'):
        params_shape = tf.shape(x)[-1:]
        moving_mean = tf.get_variable('mean', params_shape,
                                      initializer=tf.zeros_initializer,
                                      trainable=False)
        moving_variance = tf.get_variable('variance', params_shape,
                                          initializer=tf.ones_initializer,
                                          trainable=False)

        def mean_var_with_update():
            mean, variance = tf.nn.moments(x, tf.shape(x)[:-1], name='moments')
            with tf.control_dependencies([assign_moving_average(moving_mean, mean, decay),
                                          assign_moving_average(moving_variance, variance, decay)]):
                return tf.identity(mean), tf.identity(variance)
        mean, variance = tf.cond(train, mean_var_with_update, lambda: (moving_mean, moving_variance))
        if affine:
            beta = tf.get_variable('beta', params_shape,
                                   initializer=tf.zeros_initializer)
            gamma = tf.get_variable('gamma', params_shape,
                                    initializer=tf.ones_initializer)
            x = tf.nn.batch_normalization(x, mean, variance, beta, gamma, eps)
        else:
            x = tf.nn.batch_normalization(x, mean, variance, None, None, eps)
        return x
{% endhighlight %}


{% highlight bash %}
gem install rake && bundle install
{% endhighlight%}

\\( 1/x^{2} \\)

$$\mathrm{B}$$

$$e^{i\pi} + 1 = 0$$

$$
    \begin{matrix}
    1 & x & x^2 \\
    1 & y & y^2 \\
    1 & z & z^2 \\
    \end{matrix}
$$

$$
  \begin{pmatrix}
    a & b\\
    c & d\\
  \hline
    1 & 0\\
    0 & 1
  \end{pmatrix}
$$

$\bigl( \begin{smallmatrix} a & b \\ c & d \end{smallmatrix} \bigr)$

$$
\begin{align}
\sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\
 & = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\
 & = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\
 & = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\
 & \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)
\end{align}
$$

$$
f(n) =
\begin{cases}
n/2,  & \text{if $n$ is even} \\
3n+1, & \text{if $n$ is odd}
\end{cases}
$$

{% raw %}
$$a^2 + b^2 = c^2$$ --> note that all equations between these tags will not need escaping!
{% endraw %}

$$
\begin{array}{c|lcr}
n & \text{Left} & \text{Center} & \text{Right} \\
\hline
1 & 0.24 & 1 & 125 \\
2 & -1 & 189 & -8 \\
3 & -20 & 2000 & 1+10i
\end{array}
$$

Compare $\displaystyle \lim_{t \to 0} \int_t^1 f(t)\, dt$
versus $\lim_{t \to 0} \int_t^1 f(t)\, dt$.

$$
\begin{array}{ll}
\text{maximize}  & c^T x \\
\text{subject to}& d^T x = \alpha \\
&0 \le x \le 1.
\end{array}
$$

$$
\begin{alignat}{5}
  \max \quad        & z = &   x_1  & + & 12 x_2  &   &       &         && \\
  \mbox{s.t.} \quad &     & 13 x_1 & + & x_2     & + & 12x_3 & \geq 5  && \tag{constraint 1} \\
                    &     & x_1    &   &         & + & x_3   & \leq 16 && \tag{constraint 2} \\
                    &     & 15 x_1 & + & 201 x_2 &   &       & =    14 && \tag{constraint 3} \\
                    &     & \rlap{x_i \ge 0, i = 1, 2, 3}
\end{alignat}
$$

$$|x|, ||v|| \quad\longrightarrow\quad \lvert x\rvert, \lVert v\rVert$$

$$
\underset{j=1}{\overset{\infty}{\LARGE\mathrm K}}\frac{a_j}{b_j}=\cfrac{a_1}{b_1+\cfrac{a_2}{b_2+\cfrac{a_3}{b_3+\ddots}}}
$$

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
